import os
from pathlib import Path
from typing import Optional, Dict, Any, Iterator, AsyncIterator, List
from collections import deque
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage
from langchain_core.tools import tool
from langchain_core.output_parsers import PydanticOutputParser
from langchain_core.runnables import RunnablePassthrough
from pydantic import BaseModel, Field

from .schemas import CompressedCognitiveState
from .memory import ArtifactStore
from .memory_manager import MemoryManager
from .introspection import IntrospectionAgent
from .llm_factory import get_llm_model

def _log_llm_interaction(step_name: str, prompt: Any, response: Any):
    if os.getenv("ACC_DEBUG", "false").lower() != "true":
        return
        
    print(f"\n--- {step_name.upper()} ---")
    print("--- PROMPT ---")
    if isinstance(prompt, list):
        for msg in prompt:
            print(f"{msg.type.upper()}: {msg.content}")
    else:
        print(prompt)
    print("--- RESPONSE ---")
    print(response)
    print("==================================================")

class CognitiveCompressorModel:
    """
    èªçŸ¥åœ§ç¸®ãƒ¢ãƒ‡ãƒ« (CCM)ã€‚
    çŸ­æœŸè¨˜æ†¶(CCS)ã®æ›´æ–°ã¨ã€é•·æœŸè¨˜æ†¶(LTM)ã¸ã®ã‚¤ãƒ³ã‚¿ãƒ•ã‚§ãƒ¼ã‚¹ã‚’æ‹…ã†ã€‚
    Implementation based on: "The Cognitive Compressor: Optimized for bounded context windows"
    """

    def __init__(self, agents_context: str = "", model_name: Optional[str] = None):
        self.llm = get_llm_model(model_name=model_name, temperature=0.0)
        self.agents_context = agents_context

    def qualify_artifacts(self, current_input: str, ccs: Optional[CompressedCognitiveState], artifacts: list[str]) -> list[str]:
        """
        Qualify (Step 3): Recallã•ã‚ŒãŸæƒ…å ±ï¼ˆArtifactsï¼‰ã®é–¢é€£æ€§ã‚’è©•ä¾¡ã—ã€ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã™ã‚‹ã€‚
        """
        if not artifacts:
            return []
            
        system_prompt = """
ã‚ãªãŸã¯ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®è¨˜æ†¶é¸åˆ¥å®˜ã§ã™ã€‚
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ã¨ç¾åœ¨ã®çŠ¶æ…‹ã«åŸºã¥ãã€æ¤œç´¢ã•ã‚ŒãŸéå»ã®è¨˜æ†¶ï¼ˆArtifactsï¼‰ãŒã€Œä»Šã®å¯¾è©±ã«å¿…è¦ã‹ã©ã†ã‹ã€ã‚’åˆ¤å®šã—ã¦ãã ã•ã„ã€‚

# ç¾åœ¨ã®å…¥åŠ›
{current_input}

# ç¾åœ¨ã®çŠ¶æ…‹è¦ç´„
{ccs_gist}

# åˆ¤å®šåŸºæº–
- ç¾åœ¨ã®ã‚¿ã‚¹ã‚¯ã‚„è³ªå•ã«ç›´æ¥é–¢é€£ã™ã‚‹æƒ…å ±ã‹ï¼Ÿ
- æ–‡è„ˆã‚’è£œå®Œã™ã‚‹ãŸã‚ã«ä¸å¯æ¬ ã‹ï¼Ÿ

å¿…è¦ãªArtifactã®ã¿ã‚’ãƒªã‚¹ãƒˆã¨ã—ã¦è¿”ã—ã¦ãã ã•ã„ã€‚ä¸è¦ãªå ´åˆã¯ç©ºãƒªã‚¹ãƒˆã‚’è¿”ã—ã¦ãã ã•ã„ã€‚
"""
        prompt = ChatPromptTemplate.from_messages([
            ("system", system_prompt),
            ("human", "Artifacts: {artifacts_list}")
        ])
        
        class QualifiedList(BaseModel):
            selected: list[str] = Field(description="é–¢é€£æ€§ãŒé«˜ã„ã¨åˆ¤æ–­ã•ã‚ŒãŸArtifactã®å†…å®¹ãƒªã‚¹ãƒˆ")

        chain = prompt | self.llm.with_structured_output(QualifiedList)
        
        try:
            result = chain.invoke({
                "current_input": current_input,
                "ccs_gist": ccs.semantic_gist if ccs else "None",
                "artifacts_list": "\n---\n".join(artifacts)
            })
            
            _log_llm_interaction("STEP 3: Qualify Artifacts", prompt.format_messages(current_input=current_input, ccs_gist=ccs.semantic_gist if ccs else "None", artifacts_list="\n---\n".join(artifacts)), result.selected)
            
            return result.selected
        except Exception as e:
            if os.getenv("ACC_DEBUG", "false").lower() == "true":
                print(f"DEBUG: Qualify Artifacts Failed: {e}")
            return []

    def compress_and_commit(self, current_input: str, prev_ccs: Optional[CompressedCognitiveState], qualified_artifacts: list[str], long_term_memory: str = "") -> CompressedCognitiveState:
        """
        Compress & Commit (Step 4): æƒ…å ±ã‚’çµ±åˆã—ã¦æ–°ã—ã„CCSã‚’ç”Ÿæˆã™ã‚‹ã€‚
        """
        system_prompt = """
ã‚ãªãŸã¯ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®èªçŸ¥ç®¡ç†è€… (Cognitive Manager) ã§ã™ã€‚
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨ã®ä¼šè©±å±¥æ­´ã‚’ãã®ã¾ã¾ä¿å­˜ã™ã‚‹ã®ã§ã¯ãªãã€æ„æ€æ±ºå®šã«å¿…è¦ãªã€ŒçŠ¶æ…‹ (State)ã€ã ã‘ã‚’æ›´æ–°ã—ã¦ãã ã•ã„ã€‚

# å‹•ä½œãƒ«ãƒ¼ãƒ« (Agents Protocols)
{agents_context}

# æ—¢å­˜ã®é•·æœŸè¨˜æ†¶ (Existing Long-term Knowledge)
{long_term_memory}

æŒ‡ç¤ºï¼š
é•·æœŸè¨˜æ†¶ã«æ—¢ã«å­˜åœ¨ã™ã‚‹æƒ…å ±ã¯ã€CCSã«é‡è¤‡ã—ã¦ä¿å­˜ã—ãªã„ã§ãã ã•ã„ã€‚

# å‰å›ã®çŠ¶æ…‹ (Previous State)
{prev_state_json}

# é–¢é€£ã™ã‚‹éå»ã®è¨˜æ†¶ (Qualified Artifacts)
{artifacts}

# æ–°ã—ã„å…¥åŠ› (Current Input)
{current_input}

ã“ã‚Œã‚‰ã‚’çµ±åˆã—ã€æ–°ã—ã„ã€Œåœ§ç¸®ã•ã‚ŒãŸèªçŸ¥çŠ¶æ…‹ (CCS)ã€ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
"""
        prompt = ChatPromptTemplate.from_messages([
            ("system", system_prompt),
            ("human", "Update the Cognitive State based on the input: {current_input}")
        ])
        
        chain = prompt | self.llm.with_structured_output(CompressedCognitiveState)
        
        prev_state_json = prev_ccs.model_dump_json(indent=2) if prev_ccs else "None (Initial State)"
        artifacts_str = "\n---\n".join(qualified_artifacts) if qualified_artifacts else "None"
        
        input_vars = {
            "prev_state_json": prev_state_json,
            "artifacts": artifacts_str,
            "current_input": current_input,
            "agents_context": self.agents_context,
            "long_term_memory": long_term_memory
        }
        
        new_ccs = chain.invoke(input_vars)
        
        # Log the interaction
        _log_llm_interaction("STEP 4: Compress & Commit", prompt.format_messages(**input_vars), new_ccs)
        
        return new_ccs

class AgentEngine:
    """
    CCSã‚’å‚ç…§ã—ã¦æœ€çµ‚çš„ãªå›ç­”ã‚’ç”Ÿæˆã™ã‚‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæœ¬ä½“ã€‚
    å±¥æ­´å…¨æ–‡ã¯è¦‹ãšã€CCSã¨ç¾åœ¨ã®å…¥åŠ›ã®ã¿ã‚’è¦‹ã‚‹ã€‚
    """
    def __init__(self, store: ArtifactStore, identity_context: str = "", soul_context: str = "", user_context: str = "", agents_context: str = "", model_name: Optional[str] = None):
        self.raw_llm = get_llm_model(model_name=model_name, temperature=0.7)
        self.identity_context = identity_context
        self.soul_context = soul_context
        self.user_context = user_context
        self.agents_context = agents_context
        self.store = store

        # Define and Bind Tools
        @tool
        def search_memory(query: str) -> str:
            """
            Search the agent's long-term memory and daily notes for information.
            Use this tool when the conversation context is missing information or when you need to recall past events.
            """
            if os.getenv("ACC_DEBUG", "false").lower() == "true":
                print(f"\n[ACC] ğŸ” Searching Memory with query: '{query}'")

            results = self.store.recall(query, n_results=3)
            # recall returns list of strings, join them
            return "\n---\n".join(results) if results else "No relevant information found."

        self.tools = [search_memory]
        self.llm = self.raw_llm.bind_tools(self.tools)

    def generate_response(self, current_input: str, ccs: CompressedCognitiveState, recent_memory: str = "", history: List[BaseMessage] = []) -> str:
        # Construct System Prompt
        system_prompt = """ã‚ãªãŸã¯AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚

# ã‚ãªãŸã®ã‚¢ã‚¤ãƒ‡ãƒ³ãƒ†ã‚£ãƒ†ã‚£ (Identity)
{identity_context}

# ã‚ãªãŸã®å†…é¢ãƒ»æŒ‡é‡ (Soul)
{soul_context}

# å³æ ¼ã«å¾“ã†ã¹ããƒ«ãƒ¼ãƒ« (Agents Protocols)
{agents_context}

# ãƒ¦ãƒ¼ã‚¶ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ« (User Profile)
{user_context}

# ç›´è¿‘ã®è¨˜æ†¶ (Recent Memory)
{recent_memory}

--

ä»¥ä¸‹ã®ã€Œåœ§ç¸®ã•ã‚ŒãŸèªçŸ¥çŠ¶æ…‹ (CCS)ã€ã¨ã€Œç›´è¿‘ã®ä¼šè©±å±¥æ­´ã€ã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦æŒã¡ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«å¿œç­”ã—ã¦ãã ã•ã„ã€‚
ã‚‚ã—æƒ…å ±ãŒä¸è¶³ã—ã¦ã„ã‚‹å ´åˆã¯ã€`search_memory` ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ã¦éå»ã®è¨˜æ†¶ã‚’æ¤œç´¢ã—ã¦ãã ã•ã„ã€‚

# ç¾åœ¨ã®èªçŸ¥çŠ¶æ…‹ (Current Cognitive State)
{ccs_json}

ã“ã®çŠ¶æ…‹ã«åŸºã¥ãã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ã«å¯¾ã—ã¦é©åˆ‡ã«å¿œç­”ãƒ»ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¡Œã£ã¦ãã ã•ã„ã€‚
åˆ¶ç´„äº‹é … (Constraints) ã¯å¿…ãšå®ˆã£ã¦ãã ã•ã„ã€‚
"""
        prompt = ChatPromptTemplate.from_messages([
            ("system", system_prompt),
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "{current_input}"),
            MessagesPlaceholder(variable_name="agent_scratchpad", optional=True), # For older LC versions or manual tool loop
        ])

        # Prepare initial input vars
        input_vars = {
            "ccs_json": ccs.model_dump_json(indent=2),
            "current_input": current_input,
            "identity_context": self.identity_context,
            "soul_context": self.soul_context,
            "user_context": self.user_context,
            "agents_context": self.agents_context,
            "recent_memory": recent_memory,
            "chat_history": history
        }

        # Manual Tool Execution Loop (ReAct-like)
        messages = prompt.format_messages(**input_vars)
        
        # 1. First LLM Call
        ai_msg = self.llm.invoke(messages)
        
        _log_llm_interaction("STEP 5: Action (Initial)", messages, ai_msg)

        # Loop for tool calls
        tool_iterations = 0
        while ai_msg.tool_calls and tool_iterations < 3:
            messages.append(ai_msg)
            
            for tool_call in ai_msg.tool_calls:
                selected_tool = {"search_memory": self.tools[0]}[tool_call["name"].lower()]
                tool_output = selected_tool.invoke(tool_call["args"])
                messages.append(ToolMessage(content=tool_output, tool_call_id=tool_call["id"]))
                
                # Log tool output
                if os.getenv("ACC_DEBUG", "false").lower() == "true":
                    print(f"TOOL OUTPUT ({tool_call['name']}): {tool_output}")

            # 2. Subsequent LLM Call
            tool_iterations += 1
            ai_msg = self.llm.invoke(messages)
            _log_llm_interaction(f"STEP 5: Action (After Tool {tool_iterations})", messages, ai_msg)

        return ai_msg.content

    async def generate_response_stream(self, current_input: str, ccs: CompressedCognitiveState, recent_memory: str = "", history: List[BaseMessage] = []) -> AsyncIterator[str]:
        """
        ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ç”Ÿæˆã™ã‚‹éåŒæœŸã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ã€‚
        ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã®ãƒ«ãƒ¼ãƒ—å‡¦ç†ã‚’å«ã‚€ã€‚
        """
        system_prompt = """ã‚ãªãŸã¯AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚

# ã‚ãªãŸã®ã‚¢ã‚¤ãƒ‡ãƒ³ãƒ†ã‚£ãƒ†ã‚£ (Identity)
{identity_context}

# ã‚ãªãŸã®å†…é¢ãƒ»æŒ‡é‡ (Soul)
{soul_context}

# å³æ ¼ã«å¾“ã†ã¹ããƒ«ãƒ¼ãƒ« (Agents Protocols)
{agents_context}

# ãƒ¦ãƒ¼ã‚¶ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ« (User Profile)
{user_context}

# ç›´è¿‘ã®è¨˜æ†¶ (Recent Memory)
{recent_memory}

--

ä»¥ä¸‹ã®ã€Œåœ§ç¸®ã•ã‚ŒãŸèªçŸ¥çŠ¶æ…‹ (CCS)ã€ã¨ã€Œç›´è¿‘ã®ä¼šè©±å±¥æ­´ã€ã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦æŒã¡ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«å¿œç­”ã—ã¦ãã ã•ã„ã€‚
ã‚‚ã—æƒ…å ±ãŒä¸è¶³ã—ã¦ã„ã‚‹å ´åˆã¯ã€`search_memory` ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ã¦éå»ã®è¨˜æ†¶ã‚’æ¤œç´¢ã—ã¦ãã ã•ã„ã€‚

# ç¾åœ¨ã®èªçŸ¥çŠ¶æ…‹ (Current Cognitive State)
{ccs_json}

ã“ã®çŠ¶æ…‹ã«åŸºã¥ãã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ã«å¯¾ã—ã¦é©åˆ‡ã«å¿œç­”ãƒ»ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¡Œã£ã¦ãã ã•ã„ã€‚
åˆ¶ç´„äº‹é … (Constraints) ã¯å¿…ãšå®ˆã£ã¦ãã ã•ã„ã€‚
"""
        prompt = ChatPromptTemplate.from_messages([
            ("system", system_prompt),
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "{current_input}"),
            MessagesPlaceholder(variable_name="agent_scratchpad", optional=True), # For older LC versions or manual tool loop
        ])

        input_vars = {
            "ccs_json": ccs.model_dump_json(indent=2),
            "current_input": current_input,
            "identity_context": self.identity_context,
            "soul_context": self.soul_context,
            "user_context": self.user_context,
            "agents_context": self.agents_context,
            "recent_memory": recent_memory,
            "chat_history": history
        }

        # Manual Streaming Tool Execution Loop
        messages = prompt.format_messages(**input_vars)
        
        tool_iterations = 0
        while tool_iterations < 3:
            # 1. Stream First/Next LLM Call
            # We need to capture the full message to check for tool calls, 
            # while yielding content chunks to the user.
            
            ai_msg_content = ""
            tool_calls = []
            
            # Note: For streaming tool calls, we should ideally aggregate chunks.
            # However, simpler approach: stream, then if tool_calls attr exists on the final aggregated object (not easy with simple loop)
            # We will use 'astream' to yield chunks, and we also need to reconstruct the AIMessage.
            # Using 'astream_events' or similar is better, but here we can iterate and check chunks.
            
            current_tool_call = None
            
            _log_llm_interaction(f"STEP 5: Action Stream (Iter {tool_iterations})", messages, "(Streaming...)")

            ai_message_chunk = None
            
            async for chunk in self.llm.astream(messages):
                if not ai_message_chunk:
                    ai_message_chunk = chunk
                else:
                    ai_message_chunk += chunk
                
                if chunk.content:
                    if isinstance(chunk.content, list):
                        # Handle content being a list (e.g. multi-modal or specific provider behaviors)
                        # Normally it's a list of strings or dicts. If strings, join them.
                        content_str = ""
                        for item in chunk.content:
                            if isinstance(item, str):
                                content_str += item
                            elif isinstance(item, dict) and "text" in item:
                                content_str += item["text"]
                        
                        yield content_str
                    else:
                        yield chunk.content
                    
            # After streaming finishes for this turn, check if there were tool calls
            if ai_message_chunk and ai_message_chunk.tool_calls:
                # Tool call detected!
                messages.append(ai_message_chunk)
                
                # Notify user (optional, can look like a thought)
                yield "\n(Searching memory...)\n" 

                for tool_call in ai_message_chunk.tool_calls:
                    selected_tool = {"search_memory": self.tools[0]}[tool_call["name"].lower()]
                    tool_output = selected_tool.invoke(tool_call["args"])
                    
                    messages.append(ToolMessage(content=tool_output, tool_call_id=tool_call["id"]))
                    
                    if os.getenv("ACC_DEBUG", "false").lower() == "true":
                        print(f"TOOL OUTPUT ({tool_call['name']}): {tool_output}")

                tool_iterations += 1
                # Continue loop -> Re-invoke LLM with tool outputs
            else:
                # No tool calls, this was the final answer.
                break

class ACCController:
    """
    ACCã®ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ©ãƒ¼ã€‚
    ãƒ¡ãƒ¢ãƒªæ›´æ–°ã‚µã‚¤ã‚¯ãƒ«ã‚’åˆ¶å¾¡ã™ã‚‹ã€‚
    """
    def __init__(self):
        # Load Context Files
        self.user_name = os.getenv("ACC_USER_NAME", "edom18")
        self.settings_dir = Path(f"agent-settings/{self.user_name}")
        self.common_settings_dir = Path("agent-settings/common")
        
        self.identity_context = self._load_context_file("IDENTITY.md")
        self.soul_context = self._load_context_file("SOUL.md")
        self.user_context = self._load_context_file("USER.md")
        self.agents_context = self._load_context_file("AGENTS.md", is_common=True)

        # Initialize Memory Components
        self.memory_manager = MemoryManager(user_name=self.user_name)
        self.introspection = IntrospectionAgent(user_name=self.user_name)

        self.ccm = CognitiveCompressorModel(agents_context=self.agents_context)
        self.store = ArtifactStore()
        self.agent = AgentEngine(
            store=self.store,
            identity_context=self.identity_context,
            soul_context=self.soul_context,
            user_context=self.user_context,
            agents_context=self.agents_context
        )
        self.history: deque = deque(maxlen=15)
        self.current_ccs: Optional[CompressedCognitiveState] = None
        self.current_recent_memory: str = ""

    def _load_context_file(self, filename: str, is_common: bool = False) -> str:
        base_dir = self.common_settings_dir if is_common else self.settings_dir
        file_path = base_dir / filename
        if file_path.exists():
            return file_path.read_text(encoding="utf-8")
        return ""

    def prepare_turn(self, user_input: str) -> Dict[str, Any]:
        """
        ã‚¿ãƒ¼ãƒ³ã®æº–å‚™ãƒ•ã‚§ãƒ¼ã‚º (Recall, Qualify, Compress)ã€‚
        è¿”ã‚Šå€¤ã¨ã—ã¦ã€æ–°ã—ã„CCSã¨å–å¾—ã—ãŸã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆã‚’å«ã‚€è¾æ›¸ã‚’è¿”ã™ã€‚
        """
        # 1. Recall (Step 2)
        recall_query = f"{user_input}\nContext: {self.current_ccs.semantic_gist if self.current_ccs else ''}"
        raw_artifacts = self.store.recall(recall_query)
        
        # 2. Qualify (Step 3)
        qualified_artifacts = self.ccm.qualify_artifacts(user_input, self.current_ccs, raw_artifacts)
        
        # Load Long-term Memory for CCM
        ltm_content = self.memory_manager.read_long_term_memory()

        # 3. Compress & Commit (Step 4)
        new_ccs = self.ccm.compress_and_commit(
            user_input, 
            self.current_ccs, 
            qualified_artifacts,
            long_term_memory=ltm_content
        )
        
        # Update internal state (Replacement)
        self.current_ccs = new_ccs
        
        # Load Recent Memory for Action
        self.current_recent_memory = self.memory_manager.read_recent_daily_logs()
        
        return {
            "text": user_input, 
            "ccs": new_ccs,
            "qualified_artifacts": qualified_artifacts,
            "recent_memory": self.current_recent_memory,
            "history": list(self.history)
        }

    async def stream_action(self, user_input: str) -> AsyncIterator[str]:
        """
        ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ•ã‚§ãƒ¼ã‚º (Step 5) ã®éåŒæœŸã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å®Ÿè¡Œã€‚
        """
        async for chunk in self.agent.generate_response_stream(user_input, self.current_ccs, recent_memory=self.current_recent_memory, history=list(self.history)):
            yield chunk

    def finalize_turn(self, user_input: str, response_text: str):
        """
        ã‚¿ãƒ¼ãƒ³ã®å®Œäº†å‡¦ç†ã€‚
        æ—¥è¨˜ã®æ›´æ–°ã€è¨˜æ†¶ã®æŠ½å‡ºã€ãƒ™ã‚¯ãƒˆãƒ«DBã¸ã®ä¿å­˜ãªã©ã€é‡ã„å‡¦ç†ã‚’ã“ã“ã§è¡Œã†ã€‚
        """
        # --- Memory Updates (OpenClaw Style) ---
        
        # 1. Introspection Cycle (Journal, Facts, Context Updates)
        introspection_results = self.introspection.run_introspection_cycle(user_input, response_text, self.current_ccs)
        
        # Log Journal
        if introspection_results["journal_entry"]:
            self.memory_manager.append_daily_log(introspection_results["journal_entry"])
            
        # Log Facts
        facts = introspection_results["facts"]
        if facts:
            self.memory_manager.append_to_long_term_memory(facts)
            for fact in facts:
                self.store.add_artifact(fact, metadata={"type": "semantic_memory", "source": "memory_flush"})
        
        # Notify if Context Updated
        if introspection_results["updated_files"]:
            print(f"*** Context Updated: {introspection_results['updated_files']} ***")
            # Reload context for next turn
            if "IDENTITY.md" in introspection_results["updated_files"]:
                self.identity_context = self._load_context_file("IDENTITY.md")
                self.agent.identity_context = self.identity_context
            if "SOUL.md" in introspection_results["updated_files"]:
                self.soul_context = self._load_context_file("SOUL.md")
                self.agent.soul_context = self.soul_context
            if "USER.md" in introspection_results["updated_files"]:
                self.user_context = self._load_context_file("USER.md")
                self.agent.user_context = self.user_context
            if "AGENTS.md" in introspection_results["updated_files"]:
                self.agents_context = self._load_context_file("AGENTS.md", is_common=True)
                self.agent.agents_context = self.agents_context
                self.ccm.agents_context = self.agents_context

        # (Legacy) Episodic Trace for Artifact Store
        # ä»Šå›ã®CCSã®ã‚³ãƒ”ãƒ¼ã‚’ä¿å­˜
        self.store.add_artifact(
            content=f"User: {user_input}\nAssistant: {response_text}\nGist: {self.current_ccs.semantic_gist}",
            metadata={"type": "episodic_memory"}
        )

        # Update Sliding Window History
        self.history.append(HumanMessage(content=user_input))
        self.history.append(AIMessage(content=response_text))